<!DOCTYPE html>
<html lang="en">
	<head>
    


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">


<link rel="stylesheet" href="https://cdn.bootcss.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb" crossorigin="anonymous">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js/styles/github-gist.min.css">


<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    </head>
<body>
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-8">
                <div class="post">
  <h1>Keras</h1>
  <span class="post-date">Dec 4nd, 2017</span>
  <p>Record the problems encountered while using Keras and their solutions. The version of Keras I used is <code>2.0.8</code>.</p>

<p></p>

<h2 id="basic">Basic</h2>

<h3 id="metric">Metric</h3>

<p>As of Keras 2.0, precision and recall were removed from the master branch. We need to implement them.</p>

<p><a href="https://github.com/fchollet/keras/issues/5400">issue #5400</a> or <a href="https://github.com/fchollet/keras/commit/a56b1a55182acf061b1eb2e2c86b48193a0e88f7">Removed batchwise metrics</a></p>

<pre><code class="language-python">from keras import backend as K

 def mcor(y_true, y_pred):
     #matthews_correlation
     y_pred_pos = K.round(K.clip(y_pred, 0, 1))
     y_pred_neg = 1 - y_pred_pos
 
     y_pos = K.round(K.clip(y_true, 0, 1))
     y_neg = 1 - y_pos

     tp = K.sum(y_pos * y_pred_pos)
     tn = K.sum(y_neg * y_pred_neg)

     fp = K.sum(y_neg * y_pred_pos)
     fn = K.sum(y_pos * y_pred_neg)

     numerator = (tp * tn - fp * fn)
     denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

     return numerator / (denominator + K.epsilon())

def precision(y_true, y_pred):
    &quot;&quot;&quot;Precision metric.

    Only computes a batch-wise average of precision.

    Computes the precision, a metric for multi-label classification of
    how many selected items are relevant.
    &quot;&quot;&quot;
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def recall(y_true, y_pred):
    &quot;&quot;&quot;Recall metric.

    Only computes a batch-wise average of recall.

    Computes the recall, a metric for multi-label classification of
    how many relevant items are selected.
    &quot;&quot;&quot;
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def f1(y_true, y_pred):
    def recall(y_true, y_pred):
        &quot;&quot;&quot;Recall metric.

        Only computes a batch-wise average of recall.

        Computes the recall, a metric for multi-label classification of
        how many relevant items are selected.
        &quot;&quot;&quot;
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        &quot;&quot;&quot;Precision metric.

        Only computes a batch-wise average of precision.

        Computes the precision, a metric for multi-label classification of
        how many selected items are relevant.
        &quot;&quot;&quot;
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision
    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall))

#you can use it like this
model.compile(loss='binary_crossentropy',
              optimizer= &quot;adam&quot;,
              metrics=[mcor, recall, f1])
</code></pre>

<h2 id="problems-encountered">Problems encountered</h2>

<h3 id="2017-12-04-get-variable-s-value-in-middle-layer">2017-12-04 Â· Get variable&rsquo;s value in middle layer</h3>

<p>Reference: <a href="https://github.com/fchollet/keras/issues/41">How can I get hidden layer representation of the given data? #41</a></p>

<p>The simplest way is using the same code of the original model, and</p>

<ol>
<li>replace the output with the variable you want</li>
<li><code>new_model.set_weights(trained_model.get_weights())</code></li>
<li><code>new_model.predict(input_data, batch_size=32)</code></li>
</ol>

<p>Note the <code>batch_size</code> is import for large amount of samples. The <code>K.function()</code> mentioned in the <a href="https://github.com/fchollet/keras/issues/41">issue #41</a> raised <code>OOM exception</code>. Of course you can split data into batches by yourself and use the <code>K.function()</code> method, but the method showed above is more convinient for me in my case.</p>

<p>Example case:</p>

<p>I want to get the output value of pooling layer.</p>

<ul>
<li><code>get_model()</code> function return a model for training</li>
<li><code>train_model()</code></li>
<li><code>get_p_out()</code> function almost have the same code with <code>get_model()</code> except

<ul>
<li>the input parameters</li>
<li>the <code>Model()</code>&rsquo;s output parameter</li>
<li><code>set_weights()</code> from trained model</li>
<li>use the new model to predict value</li>
</ul></li>
</ul>

<pre><code class="language-python">def get_model(input_shape=(64, 64, 3), kernel_num=64):
    
    inputs = Input(shape=input_shape)
    
    c_out = Conv2D(filters=kernel_num, kernel_size=(3, 3), activation='relu',
                   use_bias=True, padding='same')(inputs)
    p_out = MaxPooling2D((2, 2), padding='same')(c_out)
    flat  = Flatten()(p_out)
    d_out = Dense(class_cnt, activation='softmax')(flat)
    
    model = Model(inputs, d_out)
    opt = Adam()
    model.compile(optimizer=opt, loss='categorical_crossentropy', 
                  metrics=['acc'])
    
    return model
</code></pre>

<pre><code class="language-python">def train_model(model, batch_size=32):
    
    model_path = 'model.h5'
    model_checkpoint = ModelCheckpoint(model_path, monitor='val_acc',
                                       save_best_only=True, 
                                       save_weights_only=True)
    early_stopping = EarlyStopping(patience=5, monitor='val_acc')
    hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=100,
                     validation_data=[X_test, y_test], 
                     callbacks=[model_checkpoint, early_stopping])
    
    model.load_weights(model_path)
    
    preds = model.evaluate(X_test, y_test)
    
    print 'Test loss:', preds[0]
    print 'Test accuracy:', preds[1]
    
    return model
</code></pre>

<pre><code class="language-python">def get_p_out(model_trained, input_data, 
              input_shape=(64, 64, 3), kernel_num=64):
    
    inputs = Input(shape=input_shape)
    
    c_out = Conv2D(filters=kernel_num, kernel_size=(3, 3), activation='relu',
                   use_bias=True, padding='same', )(inputs)
    p_out = MaxPooling2D((2, 2), padding='same')(c_out)
    flat  = Flatten()(p_out)
    d_out = Dense(class_cnt, activation='softmax')(flat)
    
    model = Model(inputs, p_out)
    opt = Adam()
    model.compile(optimizer=opt, loss='categorical_crossentropy',
                  metrics=['acc'])
    
    model.set_weights(model_trained.get_weights())
    
    pred = model.predict(input_data, batch_size=32)
    
    return pred
</code></pre>
</div>
            </div>
        </div>
        



<script src="https://cdn.bootcss.com/jquery/3.2.1/jquery.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdn.bootcss.com/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.bootcss.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ" crossorigin="anonymous"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


<script src="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/auto-render.min.js"></script>
<script>
    renderMathInElement(document.body);
</script>


    </div>
</body>
</html>
