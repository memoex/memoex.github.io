<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Mr.Blue</title>
    <link>https://memoex.github.io/note/tech/ml/</link>
    <description>Recent content in Machine Learning on Mr.Blue</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>silverhugh.77@gmail.com (Mr.Blue)</managingEditor>
    <webMaster>silverhugh.77@gmail.com (Mr.Blue)</webMaster>
    <lastBuildDate>Sun, 15 Apr 2018 13:42:36 +0800</lastBuildDate>
    
	<atom:link href="https://memoex.github.io/note/tech/ml/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Machine Learning</title>
      <link>https://memoex.github.io/note/tech/ml/ml/</link>
      <pubDate>Wed, 18 Apr 2018 09:05:03 +0800</pubDate>
      <author>silverhugh.77@gmail.com (Mr.Blue)</author>
      <guid>https://memoex.github.io/note/tech/ml/ml/</guid>
      <description> This machine learning note records some important concepts and some most used methods, including:
 Bias variance tradeoff  Bias variance tradeoff </description>
    </item>
    
    <item>
      <title>Image</title>
      <link>https://memoex.github.io/note/tech/ml/image/</link>
      <pubDate>Wed, 31 Jan 2018 20:25:38 +0800</pubDate>
      <author>silverhugh.77@gmail.com (Mr.Blue)</author>
      <guid>https://memoex.github.io/note/tech/ml/image/</guid>
      <description>&lt;p&gt;Notes about image processing.&lt;/p&gt;

&lt;h2 id=&#34;modules&#34;&gt;Modules&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://memoex.github.io/note/tech/ml/datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://memoex.github.io/note/tech/ml/augmentation&#34;&gt;Data Augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://memoex.github.io/note/tech/ml/metrics&#34;&gt;Metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://memoex.github.io/note/tech/ml/loss&#34;&gt;Loss Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://memoex.github.io/note/tech/ml/optimization&#34;&gt;Optimization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Datasets</title>
      <link>https://memoex.github.io/note/tech/ml/datasets/</link>
      <pubDate>Thu, 29 Mar 2018 20:32:10 +0800</pubDate>
      <author>silverhugh.77@gmail.com (Mr.Blue)</author>
      <guid>https://memoex.github.io/note/tech/ml/datasets/</guid>
      <description> Image datasets Minist ImageNet  ImageNet labels  </description>
    </item>
    
    <item>
      <title>Data Augmentation</title>
      <link>https://memoex.github.io/note/tech/ml/augmentation/</link>
      <pubDate>Thu, 08 Mar 2018 22:06:06 +0800</pubDate>
      <author>silverhugh.77@gmail.com (Mr.Blue)</author>
      <guid>https://memoex.github.io/note/tech/ml/augmentation/</guid>
      <description>&lt;p&gt;Data augmentation is the process of increasing the size of a dataset by transforming it
in ways that a neural network is unlikely to learn by itself.&lt;/p&gt;

&lt;p&gt;This article will introduce:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Common data augmentation methods.&lt;/li&gt;
&lt;li&gt;Image augmentation with &lt;code&gt;imgaug&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Popular tools for data augmentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Metrics</title>
      <link>https://memoex.github.io/note/tech/ml/metrics/</link>
      <pubDate>Tue, 06 Mar 2018 11:08:39 +0800</pubDate>
      <author>silverhugh.77@gmail.com (Mr.Blue)</author>
      <guid>https://memoex.github.io/note/tech/ml/metrics/</guid>
      <description>&lt;p&gt;Sometimes it&amp;rsquo;s hard to tell the differences between precision, accuracy, recall and so on especially for newbees like me.&lt;/p&gt;

&lt;p&gt;But let&amp;rsquo;s try to distinguish them with stories.
In this article, you will see some common used metrics, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Metrics for binary classification: accuracy, precision, reacall, f1-score and so on&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Loss Function</title>
      <link>https://memoex.github.io/note/tech/ml/loss/</link>
      <pubDate>Mon, 19 Mar 2018 19:44:24 +0800</pubDate>
      <author>silverhugh.77@gmail.com (Mr.Blue)</author>
      <guid>https://memoex.github.io/note/tech/ml/loss/</guid>
      <description>&lt;p&gt;This article will introdcue:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;expected risk&lt;/li&gt;
&lt;li&gt;some common loss function&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;loss-function-in-classification&#34;&gt;Loss function in classification&lt;/h2&gt;

&lt;p&gt;The goal of classification problem, or many machine learning porblem, is given training sets \(\{(x^{(i)}, y^{(i)}); i=1,\cdots,m\}\),
to find a good predictor \(f\) so that
\(f(x^{(i)})\) is a good estimate of \(y^{(i)}\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why we need a loss function?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We need a loss function to measure how &amp;ldquo;close&amp;rdquo; of estimate value \(\hat y^{(i)}\) and the target value \(y^{(i)}\)
and we usually optimize our model by minimizing the loss.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Optimization</title>
      <link>https://memoex.github.io/note/tech/ml/optimization/</link>
      <pubDate>Thu, 12 Apr 2018 20:12:45 +0800</pubDate>
      <author>silverhugh.77@gmail.com (Mr.Blue)</author>
      <guid>https://memoex.github.io/note/tech/ml/optimization/</guid>
      <description>&lt;p&gt;This article will first introduce gradient descent, and then go through most of popular optimization methods, such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SGD&lt;/li&gt;
&lt;li&gt;RMSprop&lt;/li&gt;
&lt;li&gt;Adam&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Boosting</title>
      <link>https://memoex.github.io/note/tech/ml/boosting/</link>
      <pubDate>Tue, 17 Apr 2018 15:42:28 +0800</pubDate>
      <author>silverhugh.77@gmail.com (Mr.Blue)</author>
      <guid>https://memoex.github.io/note/tech/ml/boosting/</guid>
      <description>&lt;p&gt;This article will introduce:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What is boosting.&lt;/li&gt;
&lt;li&gt;Common boosting algorithms, including:

&lt;ul&gt;
&lt;li&gt;AdaBoost&lt;/li&gt;
&lt;li&gt;Gradient Boosting&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Forward Learning</title>
      <link>https://memoex.github.io/note/tech/ml/forward/</link>
      <pubDate>Mon, 18 Dec 2017 19:01:49 +0800</pubDate>
      <author>silverhugh.77@gmail.com (Mr.Blue)</author>
      <guid>https://memoex.github.io/note/tech/ml/forward/</guid>
      <description>&lt;p&gt;Papers read during research &lt;strong&gt;forward deep learning&lt;/strong&gt; algorithm.&lt;/p&gt;

&lt;h2 id=&#34;quick-notes&#34;&gt;Quick notes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;正推，逆推

&lt;ul&gt;
&lt;li&gt;底层精度高=&amp;gt; 高层精度高&lt;/li&gt;
&lt;li&gt;相同结构网络，精度高的网络，从中间截取进行分类是否精度也高？&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;不同粒度特征提取的结合

&lt;ul&gt;
&lt;li&gt;粒度，卷积层数？ 层数不同的实质是什么不同？&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;在调整了一些Hyper-paramenter后，大致上可以发现影响更大的参数，比如卷积核数

&lt;ul&gt;
&lt;li&gt;调整优先级：欠拟合 &amp;gt; 过拟合&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;用CIFAR-10训练时，测试集上的loss会在某次迭代中突然丢失，然后又恢复，形成一个尖刺？&lt;/li&gt;
&lt;li&gt;将问题分割成子问题，但试图用深度学习解决的问题，都不太好分割成子问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;orthogonal-bipolar-target-vectors-1&#34;&gt;Orthogonal Bipolar Target Vectors&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Can OBV construct a middle target for CNN?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A kind of target representation.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;conventional

&lt;ul&gt;
&lt;li&gt;BNV - binary: \((0, 0, 1, 0, 0)\)&lt;/li&gt;
&lt;li&gt;BPV - bipolar?: \((-1, -1, 1, -1, -1)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OBV - orthogonal bipolar vectors&lt;/li&gt;
&lt;li&gt;NOV - Non-Orthogonal Vecotrs

&lt;ul&gt;
&lt;li&gt;For fail comparision&lt;/li&gt;
&lt;li&gt;\(V_i=(\overbrace{-1 , \cdots , -1}^{i-1}, 1, \overbrace{-1 , \cdots , -1}^{n-i})\)&lt;/li&gt;
&lt;li&gt;\(cos \theta = \frac{n-2}{n}\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;degraded characters?

&lt;ul&gt;
&lt;li&gt;They use degraded license plate images as expirement data. (车牌号)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;How to generate OBV from conventional target?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>