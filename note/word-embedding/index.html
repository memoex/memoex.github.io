<!DOCTYPE html>
<html lang="en">
	<head>
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">


    </head>
<body>
    <section id="wrapper">
        <div class="post">
  <h1>Word embedding</h1>
  <span class="post-date">Dec 4nd, 2017</span>
  <p>在使用Keras的Embedding层的时候，遇到困惑，学习一下。</p>

<p></p>

<h2 id="拟合目标">拟合目标</h2>

<p>条件概率：
$$P(w_j|w_i)$$</p>

<p>二元词概率：
$$P(w_i, w_j)$$</p>

<p>互信息：
$$log \frac{P(w_i,w_j)}{P(w_i)P(w_j)}$$</p>

<p>如果两个词语义无关，分布独立，互信息为0：
$$P(w_j,w_i) \approx P(w_i)P(w_j) \to \frac{P(w_i,w_j)}{P(w_i)P(w_j)} \approx 1 \to log \frac{P(w_i,w_j)}{P(w_i)P(w_j)}$$</p>

<h2 id="参考">参考</h2>

<ul>
<li>有谁可以解释下word embedding? - 李韶华的回答 - 知乎: <a href="https://www.zhihu.com/question/32275069/answer/109446135">https://www.zhihu.com/question/32275069/answer/109446135</a></li>
<li>Mikolov的word2vec —第一个现代的词嵌入生成方法

<ul>
<li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a></li>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a></li>
</ul></li>
</ul>
</div>
    </section>
    

<script src="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/auto-render.min.js"></script>
<script>
    renderMathInElement(document.body);
</script>


</body>
</html>
